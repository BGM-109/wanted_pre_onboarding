{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be18f476",
   "metadata": {},
   "source": [
    "# 문제1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4c3196",
   "metadata": {},
   "source": [
    "### **문제 1) Tokenizer 생성하기**\n",
    "\n",
    "**1-1. `preprocessing()`**\n",
    "\n",
    "텍스트 전처리를 하는 함수입니다.\n",
    "\n",
    "- input: 여러 영어 문장이 포함된 list 입니다. ex) ['I go to school.', 'I LIKE pizza!']\n",
    "- output: 각 문장을 토큰화한 결과로, nested list 형태입니다. ex) [['i', 'go', 'to', 'school'], ['i', 'like', 'pizza']]\n",
    "- 조건 1: 입력된 문장에 대해서 소문자로의 변환과 특수문자 제거를 수행합니다.\n",
    "- 조건 2: 토큰화는 white space 단위로 수행합니다.\n",
    "    \n",
    "    \n",
    "\n",
    "**1-2. `fit()`**\n",
    "\n",
    "어휘 사전을 구축하는 함수입니다.\n",
    "\n",
    "- input: 여러 영어 문장이 포함된 list 입니다. ex) ['I go to school.', 'I LIKE pizza!']\n",
    "- 조건 1: 위에서 만든 `preprocessing` 함수를 이용하여 각 문장에 대해 토큰화를 수행합니다.\n",
    "- 조건 2: 각각의 토큰을 정수 인덱싱 하기 위한 어휘 사전(`self.word_dict`)을 생성합니다.\n",
    "    - 주어진 코드에 있는 `self.word_dict`를 활용합니다.\n",
    "    \n",
    "\n",
    "**1-3. `transform()`**\n",
    "\n",
    "어휘 사전을 활용하여 입력 문장을 정수 인덱싱하는 함수입니다.\n",
    "\n",
    "- input: 여러 영어 문장이 포함된 list입니다. ex) ['I go to school.', 'I LIKE pizza!']\n",
    "- output: 각 문장의 정수 인덱싱으로, nested list 형태입니다. ex) [[1, 2, 3, 4], [1, 5, 6]]\n",
    "- 조건 1: 어휘 사전(`self.word_dict`)에 없는 단어는 'oov'의 index로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1dc475ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "class Tokenizer():\n",
    "  def __init__(self):\n",
    "    self.word_dict = {'oov': 0}\n",
    "    self.fit_checker = False\n",
    "  \n",
    "  def preprocessing(self, sequences):\n",
    "    result = []\n",
    "    for s in sequences:\n",
    "        new_s = s.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "        arr = list(new_s.split(' '))\n",
    "        result.append(arr)\n",
    "    return result\n",
    "  \n",
    "  def fit(self, sequences):\n",
    "    self.fit_checker = False\n",
    "    arr = self.preprocessing(sequences)\n",
    "    arr2 = list(set(sum(arr, [])))\n",
    "    token_index = 1\n",
    "    for t in arr2:\n",
    "        self.word_dict.update({t: token_index})\n",
    "        token_index+=1  \n",
    "    self.fit_checker = True\n",
    "    print(f\"fitted word dict: {self.word_dict}\")\n",
    "  \n",
    "  def transform(self, sequences):\n",
    "    result = []\n",
    "    tokens = self.preprocessing(sequences)\n",
    "    if self.fit_checker:\n",
    "        for token in tokens:\n",
    "            arr = []\n",
    "            for t in token:\n",
    "                trans_t = self.word_dict.get(t)\n",
    "                if trans_t != None:\n",
    "                    arr.append(trans_t)\n",
    "                else:\n",
    "                    arr.append(self.word_dict['oov'])\n",
    "            result.append(arr)\n",
    "        \n",
    "        \n",
    "  \n",
    "        return result\n",
    "    else:\n",
    "      raise Exception(\"Tokenizer instance is not fitted yet.\")\n",
    "      \n",
    "  def fit_transform(self, sequences):\n",
    "    self.fit(sequences)\n",
    "    result = self.transform(sequences)\n",
    "    return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "793706ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문제 1-1\n",
      "전처리 결과값: [['i', 'go', 'to', 'school'], ['i', 'like', 'pizza']]\n",
      "문제 1-2\n",
      "fitted word dict: {'oov': 0, 'like': 1, 'go': 2, 'i': 3, 'to': 4, 'pizza': 5, 'school': 6}\n",
      "문제 1-3\n",
      "결과값: [[3, 2, 4, 6], [3, 1, 5]]\n"
     ]
    }
   ],
   "source": [
    "example_set = ['I go to school.', 'I LIKE pizza!']\n",
    "\n",
    "token = Tokenizer()\n",
    "\n",
    "print(\"문제 1-1\")\n",
    "arr = token.preprocessing(example_set)\n",
    "print(f\"전처리 결과값: {arr}\")\n",
    "\n",
    "print(\"문제 1-2\")\n",
    "token.fit(example_set)\n",
    "\n",
    "result = token.transform(example_set)\n",
    "print(\"문제 1-3\")\n",
    "print(f\"결과값: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160199bf",
   "metadata": {},
   "source": [
    "# 문제2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5336160",
   "metadata": {},
   "source": [
    "### **문제 2) TfidfVectorizer 생성하기**\n",
    "\n",
    "**2-1. `fit()`**\n",
    "\n",
    "입력 문장들을 이용해 IDF 행렬을 만드는 함수입니다.\n",
    "\n",
    "- input: 여러 영어 문장이 포함된 list 입니다. ex) ['I go to school.', 'I LIKE pizza!']\n",
    "- 조건 1: IDF 행렬은 list 형태입니다.\n",
    "    - ex) [토큰1에 대한 IDF 값, 토큰2에 대한 IDF 값, .... ]\n",
    "- 조건 2: IDF 값은 아래 식을 이용해 구합니다.\n",
    "    \n",
    "    $$\n",
    "    idf(d,t)=log_e(\\frac{n}{1+df(d,t)})\n",
    "    $$\n",
    "    \n",
    "    - $df(d,t)$ : 단어 t가 포함된 문장 d의 개수\n",
    "    - $n$ : 입력된 전체 문장 개수\n",
    "- 조건 3: 입력된 문장의 토큰화에는 문제 1에서 만든 Tokenizer를 사용합니다.\n",
    "    \n",
    "    \n",
    "\n",
    "**2-2. `transform()`**\n",
    "\n",
    "입력 문장들을 이용해 TF-IDF 행렬을 만드는 함수입니다.\n",
    "\n",
    "- input: 여러 영어 문장이 포함된 list입니다. ex) ['I go to school.', 'I LIKE pizza!']\n",
    "- output : nested list 형태입니다.\n",
    "    \n",
    "    ex) [[tf-idf(1, 1), tf-idf(1, 2), tf-idf(1, 3)], [tf-idf(2, 1), tf-idf(2, 2), tf-idf(2, 3)]]\n",
    "    \n",
    "    |  | 토큰1 | 토큰2 | 토큰3 |\n",
    "    | --- | --- | --- | --- |\n",
    "    | 문장1 | tf-idf(1,1) | tf-idf(1,2) | tf-idf(1,3) |\n",
    "    | 문장2 | tf-idf(2,1) | tf-idf(2,2) | tf-idf(2,3) |\n",
    "- 조건1 : 입력 문장을 이용해 TF 행렬을 만드세요.\n",
    "    - $tf(d, t)$ : 문장 d에 단어 t가 나타난 횟수\n",
    "- 조건2 : 문제 2-1( `fit()`)에서 만든 IDF 행렬과 아래 식을 이용해 TF-IDF 행렬을 만드세요\n",
    "    \n",
    "    $$\n",
    "    tf-idf(d,t) = tf(d,t) \\times idf(d,t)\n",
    "    $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fd5b93cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfVectorizer:\n",
    "  def __init__(self, tokenizer):\n",
    "    self.tokenizer = tokenizer\n",
    "    self.fit_checker = False\n",
    "  \n",
    "  def fit(self, sequences):\n",
    "    tokenized = self.tokenizer.fit_transform(sequences)\n",
    "    '''\n",
    "    문제 2-1.\n",
    "    '''\n",
    "    self.fit_checker = True\n",
    "    \n",
    "\n",
    "  def transform(self, sequences):\n",
    "    if self.fit_checker:\n",
    "      tokenized = self.tokenizer.transform(sequences)\n",
    "      '''\n",
    "      문제 2-2.\n",
    "      '''\n",
    "      return self.tfidf_matrix\n",
    "    else:\n",
    "      raise Exception(\"TfidfVectorizer instance is not fitted yet.\")\n",
    "\n",
    "  \n",
    "  def fit_transform(self, sequences):\n",
    "    self.fit(sequences)\n",
    "    return self.transform(sequences)\n",
    "  def get_idf(word, seq):\n",
    "        word_dict = self.tokenizer.word_dict \n",
    "        target_token = word_dict[word]\n",
    "        value = 0\n",
    "        for s in seq:\n",
    "            if target_token in s:\n",
    "                value += 1;\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "        result = len(seq) / (1 + value)\n",
    "        return pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0087ae75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9ed255b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['She saw no irony asking me to change but wanting me to accept her for who she is.', 'He was all business when he wore his clown suit.', 'He stepped gingerly onto the bridge knowing that enchantment awaited on the other side.', \"It doesn't sound like that will ever be on my travel list.\", 'The near-death experience brought new ideas to light.', 'She found it strange that people use their cellphones to actually talk to one another.', 'Poison ivy grew through the fence they said was impenetrable.', \"She wrote him a long letter, but he didn't read it.\", 'Karen believed all traffic laws should be obeyed by all except herself.', 'His thought process was on so many levels that he gave himself a phobia of heights.', 'As the asteroid hurtled toward earth, Becky was upset her dentist appointment had been canceled.', 'The urgent care center was flooded with patients after the news of a new deadly virus was made public.', 'When motorists sped in and out of traffic, all she could think of was those in need of a transplant.', 'He decided to fake his disappearance to avoid jail.', 'He decided to live his life by the big beats manifesto.', 'Tom got a small piece of pie.', 'Blue sounded too cold at the time and yet it seemed to work for gin.', 'She can live her life however she wants as long as she listens to what I have to say.', 'As the rental car rolled to a stop on the dark road, her fear increased by the moment.', 'The ants enjoyed the barbecue more than the family.', \"You realize you're not alone as you sit in your bedroom massaging your calves after a long day of playing tug-of-war with Grandpa Joe in the hospital.\", \"He picked up trash in his spare time to dump in his neighbor's yard.\", 'Hit me with your pet shark!', 'I don’t respect anybody who can’t tell the difference between Pepsi and Coke.', 'The tattered work gloves speak of the many hours of hard labor he endured throughout his life.', 'Tuesdays are free if you bring a gnome costume.', 'He loved eating his bananas in hot dog buns.', 'The dead trees waited to be ignited by the smallest spark and seek their revenge.', 'The shark-infested South Pine channel was the only way in or out.', 'The fence was confused about whether it was supposed to keep things in or keep things out.', 'The door swung open to reveal pink giraffes and red elephants.', \"The most exciting eureka moment I've had was when I realized that the instructions on food packets were just guidelines.\", 'The pigs were insulted that they were named hamburgers.', 'So long and thanks for the fish.', \"When money was tight, he'd get his lunch money from the local wishing well.\", 'If you really strain your ears, you can just about hear the sound of no one giving a damn.', 'We should play with legos at camp.', 'Pantyhose and heels are an interesting choice of attire for the beach.', 'Please wait outside of the house.', 'Waffles are always better without fire ants and fleas.', 'There was coal in his stocking and he was thrilled.', 'She was only made the society president because she can whistle with her toes.', 'He felt that dining on the bridge brought romance to his relationship with his cat.', 'Peanut butter and jelly caused the elderly lady to think about her past.', 'He kept telling himself that one day it would all somehow make sense.', 'Patricia loves the sound of nails strongly pressed against the chalkboard.', 'The family’s excitement over going to Disneyland was crazier than she anticipated.', \"Douglas figured the best way to succeed was to do the opposite of what he'd been doing all his life.\", \"The hummingbird's wings blurred while it eagerly sipped the sugar water from the feeder.\", \"The worst thing about being at the top of the career ladder is that there's a long way to fall.\"]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "with open(f'{cwd}/random_sentences.txt') as f:\n",
    "    test_set = f.read().splitlines() \n",
    "    \n",
    "print(test_set)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
