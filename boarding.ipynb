{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be18f476",
   "metadata": {},
   "source": [
    "# 문제1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4c3196",
   "metadata": {},
   "source": [
    "### **문제 1) Tokenizer 생성하기**\n",
    "\n",
    "**1-1. `preprocessing()`**\n",
    "\n",
    "텍스트 전처리를 하는 함수입니다.\n",
    "\n",
    "- input: 여러 영어 문장이 포함된 list 입니다. ex) ['I go to school.', 'I LIKE pizza!']\n",
    "- output: 각 문장을 토큰화한 결과로, nested list 형태입니다. ex) [['i', 'go', 'to', 'school'], ['i', 'like', 'pizza']]\n",
    "- 조건 1: 입력된 문장에 대해서 소문자로의 변환과 특수문자 제거를 수행합니다.\n",
    "- 조건 2: 토큰화는 white space 단위로 수행합니다.\n",
    "    \n",
    "    \n",
    "\n",
    "**1-2. `fit()`**\n",
    "\n",
    "어휘 사전을 구축하는 함수입니다.\n",
    "\n",
    "- input: 여러 영어 문장이 포함된 list 입니다. ex) ['I go to school.', 'I LIKE pizza!']\n",
    "- 조건 1: 위에서 만든 `preprocessing` 함수를 이용하여 각 문장에 대해 토큰화를 수행합니다.\n",
    "- 조건 2: 각각의 토큰을 정수 인덱싱 하기 위한 어휘 사전(`self.word_dict`)을 생성합니다.\n",
    "    - 주어진 코드에 있는 `self.word_dict`를 활용합니다.\n",
    "    \n",
    "\n",
    "**1-3. `transform()`**\n",
    "\n",
    "어휘 사전을 활용하여 입력 문장을 정수 인덱싱하는 함수입니다.\n",
    "\n",
    "- input: 여러 영어 문장이 포함된 list입니다. ex) ['I go to school.', 'I LIKE pizza!']\n",
    "- output: 각 문장의 정수 인덱싱으로, nested list 형태입니다. ex) [[1, 2, 3, 4], [1, 5, 6]]\n",
    "- 조건 1: 어휘 사전(`self.word_dict`)에 없는 단어는 'oov'의 index로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7265e7f2-6ae6-42b0-aafe-3963a28c5a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "# 모듈 선언\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "with open(f'{cwd}/random_sentences.txt') as f:\n",
    "    test_set = f.read().splitlines() \n",
    "docs = test_set[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dc475ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "  def __init__(self):\n",
    "    self.word_dict = {'oov': 0}\n",
    "    self.fit_checker = False\n",
    "  \n",
    "  def preprocessing(self, sequences):\n",
    "    result = []\n",
    "    for s in sequences:\n",
    "        new_s = s.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "        result.append(list(new_s.split(' ')))\n",
    "    return result\n",
    "  \n",
    "  def fit(self, sequences):\n",
    "    self.fit_checker = False\n",
    "    arr = self.preprocessing(sequences)\n",
    "    fitted_docs = list(set(sum(arr, [])))\n",
    "    token_index = 1\n",
    "    for word in fitted_docs:\n",
    "        self.word_dict[word] = token_index\n",
    "        token_index+=1  \n",
    "    self.fit_checker = True\n",
    "    print(f\"fitted word dict: {self.word_dict}\")\n",
    "  \n",
    "  def transform(self, sequences):\n",
    "    result = []\n",
    "    fitted_docs = self.preprocessing(sequences)\n",
    "    if self.fit_checker:\n",
    "        for doc in fitted_docs:\n",
    "            arr = [self.word_dict[w]  if self.word_dict[w] != None else self.word_dict['oov'] for w in doc]\n",
    "            result.append(arr)\n",
    "        return result\n",
    "    else:\n",
    "      raise Exception(\"Tokenizer instance is not fitted yet.\")\n",
    "      \n",
    "  def fit_transform(self, sequences):\n",
    "    self.fit(sequences)\n",
    "    result = self.transform(sequences)\n",
    "    return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "793706ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문제 1-1\n",
      "전처리 결과값: [['i', 'go', 'to', 'school'], ['i', 'like', 'pizza']]\n",
      "문제 1-2\n",
      "fitted word dict: {'oov': 0, 'i': 1, 'to': 2, 'pizza': 3, 'go': 4, 'school': 5, 'like': 6}\n",
      "문제 1-3\n",
      "결과값: [[1, 4, 2, 5], [1, 6, 3]]\n"
     ]
    }
   ],
   "source": [
    "example_set = ['I go to school.', 'I LIKE pizza!']\n",
    "\n",
    "token = Tokenizer()\n",
    "\n",
    "print(\"문제 1-1\")\n",
    "arr = token.preprocessing(example_set)\n",
    "print(f\"전처리 결과값: {arr}\")\n",
    "\n",
    "print(\"문제 1-2\")\n",
    "token.fit(example_set)\n",
    "\n",
    "result = token.transform(example_set)\n",
    "print(\"문제 1-3\")\n",
    "print(f\"결과값: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160199bf",
   "metadata": {},
   "source": [
    "# 문제2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5336160",
   "metadata": {},
   "source": [
    "### **문제 2) TfidfVectorizer 생성하기**\n",
    "\n",
    "**2-1. `fit()`**\n",
    "\n",
    "입력 문장들을 이용해 IDF 행렬을 만드는 함수입니다.\n",
    "\n",
    "- input: 여러 영어 문장이 포함된 list 입니다. ex) ['I go to school.', 'I LIKE pizza!']\n",
    "- 조건 1: IDF 행렬은 list 형태입니다.\n",
    "    - ex) [토큰1에 대한 IDF 값, 토큰2에 대한 IDF 값, .... ]\n",
    "- 조건 2: IDF 값은 아래 식을 이용해 구합니다.\n",
    "    \n",
    "    $$\n",
    "    idf(d,t)=log_e(\\frac{n}{1+df(d,t)})\n",
    "    $$\n",
    "    \n",
    "    - $df(d,t)$ : 단어 t가 포함된 문장 d의 개수\n",
    "    - $n$ : 입력된 전체 문장 개수\n",
    "- 조건 3: 입력된 문장의 토큰화에는 문제 1에서 만든 Tokenizer를 사용합니다.\n",
    "    \n",
    "    \n",
    "\n",
    "**2-2. `transform()`**\n",
    "\n",
    "입력 문장들을 이용해 TF-IDF 행렬을 만드는 함수입니다.\n",
    "\n",
    "- input: 여러 영어 문장이 포함된 list입니다. ex) ['I go to school.', 'I LIKE pizza!']\n",
    "- output : nested list 형태입니다.\n",
    "    \n",
    "    ex) [[tf-idf(1, 1), tf-idf(1, 2), tf-idf(1, 3)], [tf-idf(2, 1), tf-idf(2, 2), tf-idf(2, 3)]]\n",
    "    \n",
    "    |  | 토큰1 | 토큰2 | 토큰3 |\n",
    "    | --- | --- | --- | --- |\n",
    "    | 문장1 | tf-idf(1,1) | tf-idf(1,2) | tf-idf(1,3) |\n",
    "    | 문장2 | tf-idf(2,1) | tf-idf(2,2) | tf-idf(2,3) |\n",
    "- 조건1 : 입력 문장을 이용해 TF 행렬을 만드세요.\n",
    "    - $tf(d, t)$ : 문장 d에 단어 t가 나타난 횟수\n",
    "- 조건2 : 문제 2-1( `fit()`)에서 만든 IDF 행렬과 아래 식을 이용해 TF-IDF 행렬을 만드세요\n",
    "    \n",
    "    $$\n",
    "    tf-idf(d,t) = tf(d,t) \\times idf(d,t)\n",
    "    $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "882bb43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfVectorizer:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.fit_checker = False\n",
    "  \n",
    "    def fit(self, sequences):\n",
    "        tokenized = self.tokenizer.fit_transform(sequences)\n",
    "        idf_dict = {}\n",
    "        for token in tokenized:\n",
    "            for t in token:\n",
    "                for word, value in self.tokenizer.word_dict.items():\n",
    "                    if t == value:\n",
    "                        idf_dict[word] = idf_dict.get(word, 0) + 1\n",
    "                    else:\n",
    "                        continue\n",
    "        print(idf_dict)  \n",
    "            \n",
    "        \n",
    "        self.fit_checker = True\n",
    "    \n",
    "\n",
    "    def transform(self, sequences):\n",
    "        tf_dict = {}\n",
    "        if self.fit_checker:\n",
    "            tokenized = self.tokenizer.transform(sequences)\n",
    "             \n",
    "            return self.tfidf_matrix\n",
    "        else:\n",
    "            raise Exception(\"TfidfVectorizer instance is not fitted yet.\")\n",
    "\n",
    "  \n",
    "    def fit_transform(self, sequences):\n",
    "        self.fit(sequences)\n",
    "        return self.transform(sequences)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d5e1563-c720-4168-a669-b0e49ad237ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitted word dict: {'oov': 0, 'to': 1, 'ever': 2, 'sound': 3, 'ideas': 4, 'me': 5, 'list': 6, 'experience': 7, 'knowing': 8, 'on': 9, 'saw': 10, 'wanting': 11, 'suit': 12, 'other': 13, 'like': 14, 'onto': 15, 'is': 16, 'it': 17, 'his': 18, 'the': 19, 'awaited': 20, 'gingerly': 21, 'business': 22, 'clown': 23, 'she': 24, 'accept': 25, 'stepped': 26, 'will': 27, 'enchantment': 28, 'was': 29, 'wore': 30, 'irony': 31, 'travel': 32, 'neardeath': 33, 'no': 34, 'all': 35, 'asking': 36, 'he': 37, 'be': 38, 'her': 39, 'brought': 40, 'light': 41, 'when': 42, 'who': 43, 'bridge': 44, 'for': 45, 'doesnt': 46, 'my': 47, 'side': 48, 'new': 49, 'but': 50, 'change': 51, 'that': 52}\n",
      "{'she': 2, 'saw': 1, 'no': 1, 'irony': 1, 'asking': 1, 'me': 2, 'to': 3, 'change': 1, 'but': 1, 'wanting': 1, 'accept': 1, 'her': 1, 'for': 1, 'who': 1, 'is': 1, 'he': 3, 'was': 1, 'all': 1, 'business': 1, 'when': 1, 'wore': 1, 'his': 1, 'clown': 1, 'suit': 1, 'stepped': 1, 'gingerly': 1, 'onto': 1, 'the': 3, 'bridge': 1, 'knowing': 1, 'that': 2, 'enchantment': 1, 'awaited': 1, 'on': 2, 'other': 1, 'side': 1, 'it': 1, 'doesnt': 1, 'sound': 1, 'like': 1, 'will': 1, 'ever': 1, 'be': 1, 'my': 1, 'travel': 1, 'list': 1, 'neardeath': 1, 'experience': 1, 'brought': 1, 'new': 1, 'ideas': 1, 'light': 1}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tf_idf = TfidfVectorizer(tokenizer)\n",
    "tf_idf.fit(docs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205ef41d",
   "metadata": {},
   "source": [
    "# IDF 구현하기\n",
    "\n",
    "DF는 문서빈도 Document Frequency\n",
    "이 값의 역수를 IDF라고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ed255b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd5517a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  IDF\n",
      "accept       0.916291\n",
      "all          0.916291\n",
      "asking       0.916291\n",
      "awaited      0.916291\n",
      "be           0.916291\n",
      "bridge       0.916291\n",
      "brought      0.916291\n",
      "business     0.916291\n",
      "but          0.916291\n",
      "change       0.916291\n",
      "clown        0.916291\n",
      "doesnt       1.609438\n",
      "enchantment  0.916291\n",
      "ever         0.916291\n",
      "experience   0.916291\n",
      "for          0.916291\n",
      "gingerly     0.916291\n",
      "he           0.000000\n",
      "her          0.510826\n",
      "his          0.916291\n",
      "ideas        0.916291\n",
      "irony        0.916291\n",
      "is           0.223144\n",
      "it           0.510826\n",
      "knowing      0.916291\n",
      "light        0.916291\n",
      "like         0.916291\n",
      "list         0.916291\n",
      "me           0.510826\n",
      "my           0.916291\n",
      "neardeath    1.609438\n",
      "new          0.916291\n",
      "no           0.510826\n",
      "on           0.223144\n",
      "onto         0.916291\n",
      "other        0.916291\n",
      "saw          0.916291\n",
      "she          0.916291\n",
      "side         0.916291\n",
      "sound        0.916291\n",
      "stepped      0.916291\n",
      "suit         0.916291\n",
      "that         0.510826\n",
      "the          0.916291\n",
      "to           0.223144\n",
      "travel       0.916291\n",
      "wanting      0.916291\n",
      "was          0.916291\n",
      "when         0.916291\n",
      "who          0.916291\n",
      "will         0.916291\n",
      "wore         0.916291\n"
     ]
    }
   ],
   "source": [
    "def get_word_list(docs):\n",
    "    result = []\n",
    "    for doc in docs:\n",
    "        result += list(set(doc.lower().translate(str.maketrans('', '', string.punctuation)).split(' ')))\n",
    "    result = set(result)\n",
    "    return sorted(list(result));\n",
    "\n",
    "def get_idf(t, docs):\n",
    "    n = len(docs)\n",
    "    df = sum([1 for doc in docs if t in doc])\n",
    "    return math.log(n / (1 + df))\n",
    "# pythonic\n",
    "# return math.log(len(docs) / (1 + sum([1 for doc in docs if t in doc])))\n",
    "\n",
    "word_list = get_word_list(docs)\n",
    "idf_table = []\n",
    "\n",
    "for word in word_list:\n",
    "    value = get_idf(word, docs)\n",
    "    idf_table.append(value)\n",
    "\n",
    "idf_table_pd = pd.DataFrame(idf_table, index = word_list, columns = [\"IDF\"])\n",
    "print(idf_table_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea333cf5",
   "metadata": {},
   "source": [
    "## TF 구현하기\n",
    "2-2 framsform()의 \n",
    "- 조건1 : 입력 문장을 이용해 TF 행렬을 만드세요.\n",
    "    - $tf(d, t)$ : 문장 d에 단어 t가 나타난 횟수\n",
    "    \n",
    "를 구해보겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "70b35a38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   accept  all  asking  awaited  be  bridge  brought  business  but  change  \\\n",
      "0       1    0       1        0   0       0        0         0    1       1   \n",
      "1       0    1       0        0   0       0        0         1    0       0   \n",
      "2       0    0       0        1   0       1        0         0    0       0   \n",
      "3       0    0       0        0   1       0        0         0    0       0   \n",
      "4       0    0       0        0   0       0        1         0    0       0   \n",
      "\n",
      "   ...  that  the  to  travel  wanting  was  when  who  will  wore  \n",
      "0  ...     0    0   2       0        1    0     0    1     0     0  \n",
      "1  ...     0    0   0       0        0    1     1    0     0     1  \n",
      "2  ...     1    3   1       0        0    0     0    0     0     0  \n",
      "3  ...     1    0   0       1        0    0     0    0     1     0  \n",
      "4  ...     0    0   1       0        0    0     0    0     0     0  \n",
      "\n",
      "[5 rows x 52 columns]\n"
     ]
    }
   ],
   "source": [
    "tf_table = []\n",
    "\n",
    "for doc in docs:\n",
    "    row = []\n",
    "    for word in word_list:\n",
    "        row.append(doc.count(word))\n",
    "    else:\n",
    "        tf_table.append(row)\n",
    "\n",
    "tf_table_result = pd.DataFrame(tf_table, columns = word_list)\n",
    "print(tf_table_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95842dcb",
   "metadata": {},
   "source": [
    "# 문제풀이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5e42b802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitted word dict: {'oov': 0, 'other': 1, 'her': 2, 'doesnt': 3, 'wanting': 4, 'it': 5, 'but': 6, 'wore': 7, 'side': 8, 'experience': 9, 'ever': 10, 'my': 11, 'light': 12, 'who': 13, 'list': 14, 'she': 15, 'his': 16, 'is': 17, 'was': 18, 'like': 19, 'accept': 20, 'sound': 21, 'business': 22, 'change': 23, 'to': 24, 'the': 25, 'no': 26, 'saw': 27, 'bridge': 28, 'for': 29, 'will': 30, 'be': 31, 'knowing': 32, 'me': 33, 'irony': 34, 'ideas': 35, 'he': 36, 'brought': 37, 'neardeath': 38, 'that': 39, 'asking': 40, 'enchantment': 41, 'suit': 42, 'awaited': 43, 'on': 44, 'new': 45, 'onto': 46, 'clown': 47, 'all': 48, 'stepped': 49, 'gingerly': 50, 'when': 51, 'travel': 52}\n",
      "[[15, 27, 26, 34, 40, 33, 24, 23, 6, 4, 33, 24, 20, 2, 29, 13, 15, 17], [36, 18, 48, 22, 51, 36, 7, 16, 47, 42], [36, 49, 50, 46, 25, 28, 32, 39, 41, 43, 44, 25, 1, 8], [5, 3, 21, 19, 39, 30, 10, 31, 44, 11, 52, 14], [25, 38, 9, 37, 45, 35, 24, 12]]\n",
      "oov 0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'in <string>' requires string as left operand, not int",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [58]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer()\n\u001b[1;32m      2\u001b[0m tf_idf \u001b[38;5;241m=\u001b[39m TfidfVectorizer(tokenizer)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtf_idf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [57]\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit\u001b[0;34m(self, sequences)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mword_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28mprint\u001b[39m(key, value)\n\u001b[0;32m---> 12\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_idf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenized\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#         new_value = self.get_idf(value, tokenized)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#         self.idf_mat.append(new_value)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# #     테이블로보기\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#     idf_table_pd = pd.DataFrame(self.idf_mat, index = list(self.tokenizer.word_dict.values()), columns = [\"IDF\"])\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#     print(idf_table_pd)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_checker \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "Input \u001b[0;32mIn [57]\u001b[0m, in \u001b[0;36mTfidfVectorizer.get_idf\u001b[0;34m(self, t, doc)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_idf\u001b[39m(\u001b[38;5;28mself\u001b[39m, t, doc):\n\u001b[0;32m---> 36\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m math\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28mlen\u001b[39m(docs) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28msum\u001b[39m([\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m doc])))\n",
      "Input \u001b[0;32mIn [57]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_idf\u001b[39m(\u001b[38;5;28mself\u001b[39m, t, doc):\n\u001b[0;32m---> 36\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m math\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28mlen\u001b[39m(docs) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28msum\u001b[39m([\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m])))\n",
      "\u001b[0;31mTypeError\u001b[0m: 'in <string>' requires string as left operand, not int"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5506bca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
